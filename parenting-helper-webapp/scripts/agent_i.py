# scripts/agent_i.py (ë¦¬íŒ©í† ë§ ìµœì¢… ì™„ë£Œ)
import argparse
import os
import sys
import textwrap

from dotenv import load_dotenv

# --- 'lib' ë””ë ‰í„°ë¦¬ì˜ ëª¨ë“ˆì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•œ ê²½ë¡œ ì„¤ì • ---
webapp_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
dotenv_path = os.path.join(webapp_root, ".env")
load_dotenv(dotenv_path)
sys.path.append(webapp_root)

# --- 4ê°œì˜ LLM í´ë¼ì´ì–¸íŠ¸ ì„í¬íŠ¸ ---
from lib.anthropic_client import get_completion as anthropic_completion
from lib.deepseek_client import get_completion as deepseek_completion
from lib.gemini_client import get_completion as gemini_completion
from lib.openai_client import get_completion as openai_completion


def call_llm_by_name(model_name: str, messages: list):
    """ëª¨ë¸ ì´ë¦„ì— ë”°ë¼ ì ì ˆí•œ í´ë¼ì´ì–¸íŠ¸ë¥¼ í˜¸ì¶œí•˜ëŠ” ë¼ìš°í„° í•¨ìˆ˜"""
    print(f" M ëª¨ë¸ í˜¸ì¶œ: {model_name}...")

    # ëª¨ë¸ ì´ë¦„ì— ë”°ë¼ ì ì ˆí•œ í•¨ìˆ˜ë¥¼ ë§¤í•‘
    if "gpt" in model_name.lower() or "openai" in model_name.lower():
        return openai_completion(messages)
    elif "claude" in model_name.lower():
        return anthropic_completion(messages)
    elif "gemini" in model_name.lower():
        return gemini_completion(messages)
    elif "deepseek" in model_name.lower():
        return deepseek_completion(messages)
    else:
        # ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ëª…ì´ë©´ ê¸°ë³¸ê°’(OpenAI)ìœ¼ë¡œ í˜¸ì¶œ
        print(
            f"ê²½ê³ : ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ '{model_name}'. ê¸°ë³¸ê°’ì¸ OpenAI GPT-4o-minië¡œ í˜¸ì¶œí•©ë‹ˆë‹¤."
        )
        return openai_completion(messages)


# --- [ìˆ˜ì •ë¨] í•¨ìˆ˜ê°€ ì¸ìë¥¼ ê°œë³„ì ìœ¼ë¡œ ë°›ë„ë¡ ë³€ê²½ ---
def handle_debug_mode(persona_path, user_input, bad_output, model_name):
    """'debug' ëª¨ë“œì˜ í•µì‹¬ ë¡œì§. ì§„ë‹¨ ë¦¬í¬íŠ¸ë¥¼ 'ë°˜í™˜'í•©ë‹ˆë‹¤."""
    print(f"ğŸ•µï¸  'debug' ëª¨ë“œ ì‹¤í–‰... {model_name}ì´ ì›ì¸ ì§„ë‹¨ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    try:
        with open(persona_path, "r", encoding="utf-8") as f:
            persona_content = f.read()
    except FileNotFoundError:
        return f"âŒ ì˜¤ë¥˜: í˜ë¥´ì†Œë‚˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ -> {persona_path}"

    system_prompt = """
    ë‹¹ì‹ ì€ LLM í˜ë¥´ì†Œë‚˜ì˜ ë¬¸ì œì ì„ ì§„ë‹¨í•˜ëŠ” 'í”„ë¡¬í”„íŠ¸ ë””ë²„ê¹… ì „ë¬¸ AI'ì…ë‹ˆë‹¤.
    ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì•„ë˜ [ë¶„ì„ ë‹¨ê³„]ë¥¼ ë°˜ë“œì‹œ ìˆœì„œëŒ€ë¡œ ë”°ë¼ ê¹Šì´ ìˆê²Œ ì‚¬ê³ í•˜ì—¬ 'ì§„ë‹¨ ë¦¬í¬íŠ¸'ë¥¼ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
    [ë¶„ì„ ë‹¨ê³„]
    1. **ì‚¬ìš©ì ì˜ë„ íŒŒì•…**: [ì‚¬ìš©ì ì…ë ¥]ì„ ë¨¼ì € ë¶„ì„í•©ë‹ˆë‹¤. ì´ê²ƒì´ ê¸´ê¸‰í•œ ì˜í•™ì  ì§ˆë¬¸ì¸ì§€, ì¼ë°˜ì ì¸ ì •ë³´ ë¬¸ì˜ì¸ì§€, ê°ì •ì  ìœ„ë¡œë¥¼ êµ¬í•˜ëŠ” ê²ƒì¸ì§€ í•µì‹¬ ì˜ë„ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
    2. **ì¶œë ¥ë¬¼ê³¼ ê·œì¹™ ë¹„êµ**: [ì‚¬ìš©ì ì˜ë„]ë¥¼ ì—¼ë‘ì— ë‘ê³ , [ì‹¤ì œ ì¶œë ¥ë¬¼]ì´ [í˜ë¥´ì†Œë‚˜ ê·œì¹™]ì˜ ê° í•­ëª©(ì—­í• , í†¤, ì¶œë ¥ í˜•ì‹ ë“±)ì„ ì–¼ë§ˆë‚˜ ì˜ ë”°ëëŠ”ì§€, í˜¹ì€ ìœ„ë°˜í–ˆëŠ”ì§€ êµ¬ì²´ì ì¸ ê·¼ê±°ë¥¼ ë“¤ì–´ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.
    3. **í•µì‹¬ ì›ì¸ ì§„ë‹¨**: 1, 2ë‹¨ê³„ ë¶„ì„ì„ ì¢…í•©í•˜ì—¬, ì™œ ì´ëŸ° ë¶€ì •í™•í•˜ê±°ë‚˜ ì•„ì‰¬ìš´ ê²°ê³¼ê°€ ë‚˜ì™”ëŠ”ì§€ ê·¼ë³¸ì ì¸ ì›ì¸ì„ ì¶”ë¡ í•©ë‹ˆë‹¤. (ì˜ˆ: ê·œì¹™ ê°„ì˜ ì¶©ëŒ, ì§€ì‹œì˜ ëª¨í˜¸ì„±, ë¶ˆí•„ìš”í•œ ì •ë³´ í¬í•¨ ë“±)
    4. **ì‹¤ìš©ì ì¸ ê¶Œì¥ ì¡°ì¹˜**: ì§„ë‹¨ëœ ì›ì¸ì„ í•´ê²°í•  ìˆ˜ ìˆëŠ”, ê°€ì¥ íš¨ê³¼ì ì´ê³  êµ¬ì²´ì ì¸ í˜ë¥´ì†Œë‚˜ ìˆ˜ì •ì•ˆì„ í•œ ë¬¸ì¥ì˜ ëª…í™•í•œ ì§€ì‹œ í˜•íƒœë¡œ ì œì•ˆí•©ë‹ˆë‹¤.
    [ì§„ë‹¨ ë¦¬í¬íŠ¸]
    (ìœ„ ë¶„ì„ ë‹¨ê³„ì— ë”°ë¼ ì‘ì„±ëœ ê²°ê³¼ë§Œ ì¶œë ¥)
    """
    user_prompt = f"[í˜ë¥´ì†Œë‚˜ ê·œì¹™]\n---\n{persona_content}\n---\n\n[ì‚¬ìš©ì ì…ë ¥]\n---\n{user_input}\n---\n\n[ì‹¤ì œ ì¶œë ¥ë¬¼]\n---\n{bad_output}\n---"
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ]

    diagnosis_report = call_llm_by_name(model_name, messages)
    return diagnosis_report


# --- [ìˆ˜ì •ë¨] í•¨ìˆ˜ê°€ ì¸ìë¥¼ ê°œë³„ì ìœ¼ë¡œ ë°›ë„ë¡ ë³€ê²½ ---
def handle_propose_mode(goal, target_path, model_name):
    """'propose' ëª¨ë“œì˜ í•µì‹¬ ë¡œì§. ì œì•ˆ ë‚´ìš©ê³¼ ì €ì¥ ê²½ë¡œë¥¼ 'ë°˜í™˜'í•©ë‹ˆë‹¤."""
    print("âœï¸  'propose' ëª¨ë“œ ì‹¤í–‰... LLMì´ ìˆ˜ì •ì•ˆ ì œì•ˆì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    try:
        with open(target_path, "r", encoding="utf-8") as f:
            original_content = f.read()
    except FileNotFoundError:
        return f"âŒ ì˜¤ë¥˜: ëŒ€ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ -> {target_path}", None

    system_prompt = """
    ë‹¹ì‹ ì€ YAML í˜•ì‹ì˜ LLM í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ìˆ˜ì •í•˜ëŠ” ì „ë¬¸ AIì…ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ 'ìˆ˜ì • ëª©í‘œ'ì™€ 'ì›ë³¸ YAML íŒŒì¼ ë‚´ìš©'ì„ ë°”íƒ•ìœ¼ë¡œ, ëª©í‘œë¥¼ ê°€ì¥ ì˜ ë‹¬ì„±í•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ YAML íŒŒì¼ ë‚´ìš©ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.
    ë‹¤ë¥¸ ì„¤ëª… ì—†ì´, ìˆ˜ì •ëœ YAML íŒŒì¼ì˜ ì „ì²´ ë‚´ìš©ë§Œ ì½”ë“œ ë¸”ë¡ ì—†ì´ ë°”ë¡œ ì¶œë ¥í•´ì•¼ í•©ë‹ˆë‹¤.
    [ ë§¤ìš° ì¤‘ìš”í•œ ê·œì¹™ ]
    - '# ì—­í• ', '# í†¤', '# ì¶œë ¥ í˜•ì‹' ë“±ì€ LLMì—ê²Œ ë‚´ë¦¬ëŠ” 'ì§€ì‹œë¬¸' ì„¹ì…˜ì…ë‹ˆë‹¤.
    - ì ˆëŒ€ë¡œ ì´ 'ì§€ì‹œë¬¸' ì„¹ì…˜ì„ ì‹¤ì œ ì‘ë‹µ ì˜ˆì‹œë¡œ ì±„ìš°ì§€ ë§ˆì‹­ì‹œì˜¤.
    - ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì§€ì‹œë¬¸ì˜ 'ë‚´ìš©'ì„ ëª©í‘œì— ë§ê²Œ ê°œì„ í•˜ëŠ” ê²ƒì´ì§€, ì§€ì‹œë¬¸ì„ ìˆ˜í–‰í•œ ì˜ˆì‹œë¥¼ ì‘ì„±í•˜ëŠ” ê²ƒì´ ì•„ë‹™ë‹ˆë‹¤.
    """
    user_prompt = (
        f"[ìˆ˜ì • ëª©í‘œ]\n{goal}\n\n[ì›ë³¸ YAML íŒŒì¼ ë‚´ìš©]\n---\n{original_content}\n---"
    )
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ]

    proposed_content = call_llm_by_name(model_name, messages)
    output_path = target_path + ".proposed"
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(proposed_content)

    return proposed_content, output_path


def handle_debate_mode(args):
    """'debate' ëª¨ë“œ. ì—¬ëŸ¬ ëª¨ë¸ì˜ ì§„ë‹¨ì„ ë°›ê³  êµì°¨ ê²€ì¦ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    print("ğŸ¤– 'debate' ëª¨ë“œ ì‹¤í–‰... ì „ë¬¸ê°€ íŒ¨ë„ í† ë¡ ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    # --- 1. ê°œë³„ ì˜ê²¬ ì·¨í•© ---
    initial_reports = {}
    print("\n--- [1ë‹¨ê³„: ê°œë³„ ì§„ë‹¨ ë¦¬í¬íŠ¸ ì·¨í•©] ---")
    # ì°¸ê³ : ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì´ ë¶€ë¶„ì„ ë¹„ë™ê¸°(asyncio)ë‚˜ ë©€í‹°ìŠ¤ë ˆë”©ìœ¼ë¡œ ì²˜ë¦¬í•˜ë©´ ë” ë¹ ë¦…ë‹ˆë‹¤.
    for model_name in args.models:
        print(f"\n>> {model_name} ì½”ì¹˜ì—ê²Œ ì§„ë‹¨ ìš”ì²­...")
        report = handle_debug_mode(args.persona, args.input, args.output, model_name)
        initial_reports[model_name] = report

    # --- 2. êµì°¨ ê²€ì¦ ë° ìµœì¢… ë³´ê³ ì„œ ìƒì„± ---
    final_report = "## ğŸ¤– Agent I ìµœì¢… í† ë¡  ë³´ê³ ì„œ\n\n"
    final_report += "### 1. ê°œë³„ ì§„ë‹¨ ë¦¬í¬íŠ¸\n\n"

    for model_name, report in initial_reports.items():
        final_report += f"#### ğŸ“„ **ì§„ë‹¨ by {model_name}**\n"
        final_report += textwrap.indent(report, "> ") + "\n\n"

    # êµì°¨ ê²€ì¦ (ê°€ì¥ ì²« ë²ˆì§¸ ë¦¬í¬íŠ¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ë¥¸ ëª¨ë¸ë“¤ì—ê²Œ ë¹„í‰ ìš”ì²­)
    if len(args.models) > 1:
        final_report += "### 2. êµì°¨ ê²€ì¦ (Cross-Examination)\n\n"

        base_model = args.models[0]
        base_report = initial_reports[base_model]
        critique_models = args.models[1:]

        final_report += f"#### ğŸ¯ **ì£¼ìš” ê²€í†  ëŒ€ìƒ: {base_model}ì˜ ì§„ë‹¨**\n"
        final_report += textwrap.indent(base_report, "> ") + "\n\n"

        critique_system_prompt = """
        ë‹¹ì‹ ì€ ë‹¤ë¥¸ AIì˜ ë¶„ì„ì„ ë‚ ì¹´ë¡­ê²Œ ë¹„í‰í•˜ëŠ” 'ìˆ˜ì„ ë¶„ì„ê°€'ì…ë‹ˆë‹¤.
        ì•„ë˜ì— ì œì‹œëœ [ë‹¤ë¥¸ AIì˜ ì§„ë‹¨ ë¦¬í¬íŠ¸]ë¥¼ ì½ê³ , ê·¸ ì§„ë‹¨ì˜ ë…¼ë¦¬ì  í—ˆì , ë†“ì¹˜ê³  ìˆëŠ” ë¶€ë¶„, ë˜ëŠ” ë” ë‚˜ì€ ëŒ€ì•ˆì´ ìˆë‹¤ë©´ ë¬´ì—‡ì¸ì§€ ë¹„í‰í•´ ì£¼ì„¸ìš”.
        ë¹„í‰ì€ ê°„ê²°í•˜ê³  í•µì‹¬ë§Œ ì§šì–´ì•¼ í•©ë‹ˆë‹¤.
        """

        for critique_model in critique_models:
            print(
                f"\n>> {critique_model} ì½”ì¹˜ì—ê²Œ {base_model}ì˜ ì§„ë‹¨ì— ëŒ€í•œ ë¹„í‰ ìš”ì²­..."
            )
            critique_user_prompt = f"[ë‹¤ë¥¸ AIì˜ ì§„ë‹¨ ë¦¬í¬íŠ¸]\n---\n{base_report}"
            messages = [
                {"role": "system", "content": critique_system_prompt},
                {"role": "user", "content": critique_user_prompt},
            ]
            critique = call_llm_by_name(critique_model, messages)
            final_report += f"#### ğŸ’¬ **ë¹„í‰ by {critique_model}**\n"
            final_report += textwrap.indent(critique, "> ") + "\n\n"

    # ìµœì¢… ë³´ê³ ì„œ ì¶œë ¥
    print("\n" + "=" * 20 + " í† ë¡  ì™„ë£Œ " + "=" * 20)
    print(final_report)
    print("=" * 52)


def main_cli():
    """í„°ë¯¸ë„ì—ì„œ ì§ì ‘ ì‹¤í–‰ë  ë•Œ ì‚¬ìš©ë˜ëŠ” CLI í•¸ë“¤ëŸ¬"""
    parser = argparse.ArgumentParser(description="Agent_I: AI í•µì‹¬ ë¡œì§ ê´€ë¦¬ ì—ì´ì „íŠ¸")
    subparsers = parser.add_subparsers(dest="mode", required=True, help="ì‹¤í–‰ ëª¨ë“œ")

    # --- 'debug' ëª¨ë“œ ì„¤ì • ---
    parser_debug = subparsers.add_parser(
        "debug", help="í˜ë¥´ì†Œë‚˜ì˜ ì‹¤íŒ¨ ì›ì¸ì„ ì§„ë‹¨í•©ë‹ˆë‹¤."
    )
    parser_debug.add_argument("--persona", type=str, required=True)
    parser_debug.add_argument("--input", type=str, required=True)
    parser_debug.add_argument("--output", type=str, required=True)
    parser_debug.add_argument(
        "--model",
        type=str,
        default="gpt-4o-mini",
        help="ì‚¬ìš©í•  LLM ëª¨ë¸ (ì˜ˆ: gpt-4o-mini, claude-3-5-haiku, gemini-1.5-flash, deepseek-chat)",
    )
    parser_debug.set_defaults(
        func=lambda args: handle_debug_mode(
            args.persona, args.input, args.output, args.model
        )
    )

    # --- 'propose' ëª¨ë“œ ì„¤ì • ---
    parser_propose = subparsers.add_parser(
        "propose", help="í˜ë¥´ì†Œë‚˜ ìˆ˜ì •ì•ˆì„ ì œì•ˆí•©ë‹ˆë‹¤."
    )
    parser_propose.add_argument("--goal", type=str, required=True)
    parser_propose.add_argument("--target", type=str, required=True)
    parser_propose.add_argument(
        "--model",
        type=str,
        default="gpt-4o-mini",
        help="ì‚¬ìš©í•  LLM ëª¨ë¸ (ì˜ˆ: gpt-4o-mini, claude-3-5-hakiu, gemini-1.5-flash, deepseek-chat)",
    )
    parser_propose.set_defaults(
        func=lambda args: handle_propose_mode(args.goal, args.target, args.model)
    )

    # --- 'debate' ëª¨ë“œ ì„œë¸ŒíŒŒì„œ ìƒì„± ---
    parser_debate = subparsers.add_parser(
        "debate", help="ì—¬ëŸ¬ ëª¨ë¸ì´ í† ë¡ í•˜ì—¬ í˜ë¥´ì†Œë‚˜ë¥¼ ì§„ë‹¨í•©ë‹ˆë‹¤."
    )
    parser_debate.add_argument("--persona", type=str, required=True)
    parser_debate.add_argument("--input", type=str, required=True)
    parser_debate.add_argument("--output", type=str, required=True)
    parser_debate.add_argument(
        "--models",
        nargs="+",
        required=True,
        help="í† ë¡ ì— ì°¸ì—¬í•  ëª¨ë¸ ëª©ë¡ (ê³µë°±ìœ¼ë¡œ êµ¬ë¶„)",
    )
    parser_debate.set_defaults(func=handle_debate_mode)

    args = parser.parse_args()

    result = args.func(args)
    if args.mode in ["debug", "propose"]:
        print("\n" + "=" * 20 + " ê²°ê³¼ " + "=" * 20)
        if args.mode == "propose":
            content, path = result
            if path:
                print(f"âœ… ì œì•ˆëœ ìˆ˜ì •ì•ˆì´ '{path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print(content)
        else:
            print(result)
        print("=" * 52)


if __name__ == "__main__":
    main_cli()
